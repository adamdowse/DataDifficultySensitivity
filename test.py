
import os
import numpy as np
import tensorflow
from tensorflow.keras import Model
from tensorflow.keras.datasets import cifar10
from tensorflow.keras.layers import Add, GlobalAveragePooling2D,Dense, Flatten, Conv2D, Lambda,	Input, BatchNormalization, Activation
from tensorflow.keras.optimizers import schedules, SGD
from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint
import wandb
import Mk2_Data as FData


# def model_configuration():
#     # Load dataset for computing dataset size
#     (input_train, _), (_, _) = load_dataset()

#     # Generic config
#     width, height, channels = 32, 32, 3
#     batch_size = 128
#     num_classes = 10
#     validation_split = 0.1 # 45/5 per the He et al. paper
#     verbose = 1
#     n = 3
#     init_fm_dim = 16
#     shortcut_type = "identity" # or: projection

#     # Dataset size
#     train_size = (1 - validation_split) * len(input_train) 
#     val_size = (validation_split) * len(input_train) 

#     # Number of steps per epoch is dependent on batch size
#     maximum_number_iterations = 64000 # per the He et al. paper
#     steps_per_epoch = tensorflow.math.floor(train_size / batch_size)
#     val_steps_per_epoch = tensorflow.math.floor(val_size / batch_size)
#     epochs = tensorflow.cast(tensorflow.math.floor(maximum_number_iterations / steps_per_epoch),dtype=tensorflow.int64)

#     # Define loss function
#     loss = tensorflow.keras.losses.CategoricalCrossentropy(from_logits=True)

#     # Learning rate config per the He et al. paper
#     boundaries = [32000, 48000]
#     values = [0.1, 0.01, 0.001]
#     lr_schedule = schedules.PiecewiseConstantDecay(boundaries, values)

#     # Set layer init
#     initializer = tensorflow.keras.initializers.HeNormal()

#     # Define optimizer
#     optimizer_momentum = 0.9
#     optimizer_additional_metrics = ["accuracy"]
#     optimizer = SGD(learning_rate=lr_schedule, momentum=optimizer_momentum)

#     # Load ModelCheckpoint callback
#     wandbcallback = wandb.keras.WandbCallback(save_model=False)

#     # Add callbacks to list
#     callbacks = [wandbcallback]

#     # Create config dictionary
#     config = {
#         "width": width,
#         "height": height,
#         "dim": channels,
#         "batch_size": batch_size,
#         "num_classes": num_classes,
#         "validation_split": validation_split,
#         "verbose": verbose,
#         "stack_n": n,
#         "initial_num_feature_maps": init_fm_dim,
#         "training_ds_size": train_size,
#         "steps_per_epoch": steps_per_epoch,
#         "val_steps_per_epoch": val_steps_per_epoch,
#         "num_epochs": epochs,
#         "loss": loss,
#         "optim": optimizer,
#         "optim_learning_rate_schedule": lr_schedule,
#         "optim_momentum": optimizer_momentum,
#         "optim_additional_metrics": optimizer_additional_metrics,
#         "initializer": initializer,
#         "callbacks": callbacks,
#         "shortcut_type": shortcut_type
#     }

#     return config

# def load_dataset():
#     """
#     Load the CIFAR-10 dataset
#     """
#     return cifar10.load_data()


# def random_crop(img, random_crop_size):
#     # Note: image_data_format is 'channel_last'
#     # SOURCE: https://jkjung-avt.github.io/keras-image-cropping/
#     assert img.shape[2] == 3
#     height, width = img.shape[0], img.shape[1]
#     dy, dx = random_crop_size
#     x = np.random.randint(0, width - dx + 1)
#     y = np.random.randint(0, height - dy + 1)
#     return img[y:(y+dy), x:(x+dx), :]


# def crop_generator(batches, crop_length):
#     """Take as input a Keras ImageGen (Iterator) and generate random
#     crops from the image batches generated by the original iterator.
#     SOURCE: https://jkjung-avt.github.io/keras-image-cropping/
#     """
#     while True:
#         batch_x, batch_y = next(batches)
#         batch_crops = np.zeros((batch_x.shape[0], crop_length, crop_length, 3))
#         for i in range(batch_x.shape[0]):
#             batch_crops[i] = random_crop(batch_x[i], (crop_length, crop_length))
#         yield (batch_crops, batch_y)


# def OLDpreprocessed_dataset():
#     """
#         Load and preprocess the CIFAR-10 dataset.
#     """
#     (input_train, target_train), (input_test, target_test) = load_dataset()

#     # Retrieve shape from model configuration and unpack into components
#     config = model_configuration()
#     width, height, dim = config.get("width"), config.get("height"),config.get("dim")
#     num_classes = config.get("num_classes")

#     # Data augmentation: perform zero padding on datasets
#     paddings = tensorflow.constant([[0, 0,], [4, 4], [4, 4], [0, 0]])
#     input_train = tensorflow.pad(input_train, paddings, mode="CONSTANT")

#     # Convert scalar targets to categorical ones
#     target_train = tensorflow.keras.utils.to_categorical(target_train, num_classes)
#     target_test = tensorflow.keras.utils.to_categorical(target_test, num_classes)

#     # Data generator for training data
#     train_generator = tensorflow.keras.preprocessing.image.ImageDataGenerator(
#         validation_split = config.get("validation_split"),
#         horizontal_flip = True,
#         rescale = 1./255,
#         preprocessing_function = tensorflow.keras.applications.resnet50.preprocess_input
#     )

#     # Generate training and validation batches
#     train_batches = train_generator.flow(input_train, target_train, batch_size=config.get("batch_size"), subset="training")
#     validation_batches = train_generator.flow(input_train, target_train, batch_size=config.get("batch_size"), subset="validation")
#     train_batches = crop_generator(train_batches, config.get("height"))
#     validation_batches = crop_generator(validation_batches, config.get("height"))

#     # Data generator for testing data
#     test_generator = tensorflow.keras.preprocessing.image.ImageDataGenerator(
#         preprocessing_function = tensorflow.keras.applications.resnet50.preprocess_input,
#         rescale = 1./255)

#     # Generate test batches
#     test_batches = test_generator.flow(input_test, target_test, batch_size=config.get("batch_size"))

#     return train_batches, validation_batches, test_batches

# def preprocessed_dataset():
#     #returns the CIFAR10 dataset in tf format as onehot and normalised
#     config = model_configuration()
#     (x_train, y_train), (x_test, y_test) = tensorflow.keras.datasets.cifar10.load_data()
#     train_count = len(x_train)*0.9
#     test_count = len(x_test)

#     #can reduce size of ds here to mathc paper if needed
#     x_train = x_train[:train_count]
#     y_train = y_train[:train_count]

#     x_train = tensorflow.cast(x_train, tensorflow.float32)
#     x_test = tensorflow.cast(x_test, tensorflow.float32)
    
#     #map y to one hot
#     y_train = tensorflow.keras.utils.to_categorical(y_train, 10)
#     y_test = tensorflow.keras.utils.to_categorical(y_test, 10)
#     y_train = tensorflow.cast(y_train, tensorflow.int64)
#     y_test = tensorflow.cast(y_test, tensorflow.int64)
    
#     #Convert to tf dataset
#     train_data = tf.data.Dataset.from_tensor_slices((x_train, y_train))
#     test_data = tf.data.Dataset.from_tensor_slices((x_test, y_test))

#     #augment the data
#     # augs = {'flip':'horizontal','crop':4}
#     # aug = tf.keras.Sequential()
#     # aug.add(tensorflow.keras.Input(shape=(32,32,3)))
#     # for aug_name in augs.keys():
#     #     if aug_name == 'flip':
#     #         if augs[aug_name] == "horizontal":
#     #             aug.add(tf.keras.layers.RandomFlip('horizontal'))
#     #     elif aug_name == 'crop':
#     #         params = augs[aug_name]
#     #         if params != None:
#     #             #to match torchvision transforms we need to add padding
#     #             aug.add(tf.keras.layers.ZeroPadding2D(padding=(params,params)))
#     #             aug.add(tf.keras.layers.RandomCrop(32,32))
#     #             #aug.add(tf.keras.layers.Resizing(self.config['img_size'][0],self.config['img_size'][1]))
#     #         else:
#     #             print('No crop variables provided, not cropping')

#     def aug(x):
#         padding = tensorflow.constant([[4,4],[4,4],[0,0]])
#         x = tensorflow.pad(x,padding, mode='CONSTANT')                                                                      if tensorflow.random.uniform(shape=[]) >= 0.5:                                                                          x = tensorflow.image.flip_left_right(x)
#         cs = tensorflow.random.uniform(shape=[],minval=0, maxval=40-32+1,dtype=tensorflow.int32)
#         x = x[cs:cs+32,cs:cs+32,:]
#         x = tensorflow.keras.applications.resnet50.preprocess_input(x)
#         x = x/255
#         return x
#     print('Augmenting data')
#     train_data = train_data.shuffle(train_count,reshuffle_each_iteration=True)
#     train_data = train_data.map(lambda x,y:(aug(x), y) , num_parallel_calls=tf.data.AUTOTUNE)
#     train_data = train_data.batch(config.get("batch_size")).prefetch(tf.data.AUTOTUNE)

#     test_data = test_data.batch(config.get("batch_size"))
#     test_data = test_data.map(lambda x,y:(tensorflow.keras.applications.resnet50.preprocess_input(x), y) , num_parallel_calls=tf.data.AUTOTUNE)
#     test_data = test_data.map(lambda x,y:(x/255., y) , num_parallel_calls=tf.data.AUTOTUNE)
    

#     return train_data, test_data, None, train_count, test_count

# def residual_block(x, number_of_filters, match_filter_size=False):
#     """
#         Residual block with
#     """
#     # Retrieve initializer
#     config = model_configuration()
#     initializer = config.get("initializer")

#     # Create skip connection
#     x_skip = x

#     # Perform the original mapping
#     if match_filter_size:
#         x = Conv2D(number_of_filters, kernel_size=(3, 3), strides=(2,2),kernel_initializer=initializer, padding="same")(x_skip)
#     else:
#         x = Conv2D(number_of_filters, kernel_size=(3, 3), strides=(1,1),kernel_initializer=initializer, padding="same")(x_skip)
#     x = BatchNormalization(axis=3)(x)
#     x = Activation("relu")(x)
#     x = Conv2D(number_of_filters, kernel_size=(3, 3),\
#         kernel_initializer=initializer, padding="same")(x)
#     x = BatchNormalization(axis=3)(x)

#     # Perform matching of filter numbers if necessary
#     if match_filter_size and config.get("shortcut_type") == "identity":
#         x_skip = Lambda(lambda x: tensorflow.pad(x[:, ::2, ::2, :], tensorflow.constant([[0, 0,], [0, 0], [0, 0], [number_of_filters//4, number_of_filters//4]]), mode="CONSTANT"))(x_skip)
#     elif match_filter_size and config.get("shortcut_type") == "projection":
#         x_skip = Conv2D(number_of_filters, kernel_size=(1,1),kernel_initializer=initializer, strides=(2,2))(x_skip)

#     # Add the skip connection to the regular mapping
#     x = Add()([x, x_skip])

#     # Nonlinearly activate the result
#     x = Activation("relu")(x)

#     # Return the result
#     return x

# def ResidualBlocks(x):
#     """
#         Set up the residual blocks.
#     """
#     # Retrieve values
#     config = model_configuration()

#     # Set initial filter size
#     filter_size = config.get("initial_num_feature_maps")

#     # Paper: "Then we use a stack of 6n layers (...)
#     #	with 2n layers for each feature map size."
#     # 6n/2n = 3, so there are always 3 groups.
#     for layer_group in range(3):

#         # Each block in our code has 2 weighted layers,
#         # and each group has 2n such blocks,
#         # so 2n/2 = n blocks per group.
#         for block in range(config.get("stack_n")):

#             # Perform filter size increase at every
#             # first layer in the 2nd block onwards.
#             # Apply Conv block for projecting the skip
#             # connection.
#             if layer_group > 0 and block == 0:
#                 filter_size *= 2
#                 x = residual_block(x, filter_size, match_filter_size=True)
#             else:
#                 x = residual_block(x, filter_size)

#     # Return final layer
#     return x


# def model_base(shp):
#     """
#         Base structure of the model, with residual blocks
#         attached.
#     """
#     # Get number of classes from model configuration
#     config = model_configuration()
#     initializer = model_configuration().get("initializer")

#     # Define model structure
#     # logits are returned because Softmax is pushed to loss function.
#     inputs = Input(shape=shp)
#     x = Conv2D(config.get("initial_num_feature_maps"), kernel_size=(3,3),strides=(1,1), kernel_initializer=initializer, padding="same")(inputs)
#     x = BatchNormalization()(x)
#     x = Activation("relu")(x)
#     x = ResidualBlocks(x)
#     x = GlobalAveragePooling2D()(x)
#     x = Flatten()(x)
#     outputs = Dense(config.get("num_classes"), kernel_initializer=initializer)(x)

#     return inputs, outputs

# def init_model():
#     """
#         Initialize a compiled ResNet model.
#     """
#     # Get shape from model configuration
#     config = model_configuration()

#     # Get model base
#     inputs, outputs = model_base((config.get("width"), config.get("height"),config.get("dim")))

#     # Initialize and compile model
#     model = Model(inputs, outputs, name=config.get("name"))
#     model.compile(loss=config.get("loss"),optimizer=config.get("optim"),metrics=config.get("optim_additional_metrics"))

#     # Print model summary
#     model.summary()

#     return model


# def train_model(model, train_batches, validation_batches, train_count, test_count):
#     """
#         Train an initialized model.
#     """

#     # Get model configuration
#     config = model_configuration()

#     # Fit data to model
#     model.fit(train_batches,
#                 batch_size=config.get("batch_size"),
#                 epochs=config.get("num_epochs"),
#                 verbose=config.get("verbose"),
#                 callbacks=config.get("callbacks"),
#                 validation_data=validation_batches)

#     return model


# def evaluate_model(model, test_batches):
#     """
#         Evaluate a trained model.
#     """
#     # Evaluate model
#     score = model.evaluate(test_batches, verbose=0)
#     print(f'Test loss: {score[0]} / Test accuracy: {score[1]}')

# def training_process():
#     """
#         Run the training process for the ResNet model.
#     """

#     # Get dataset
#     train_batches, validation_batches, test_batches, train_count, val_count = preprocessed_dataset()

#     # Initialize ResNet
#     resnet = init_model()

#     # Train ResNet model
#     trained_resnet = train_model(resnet, train_batches, validation_batches,train_count, val_count)

  
def t():
    x = np.arange(4*4*3).reshape(4,4,3)
    x = tensorflow.cast(x,tensorflow.float32)
    ndim = len(x.shape)
    m = [1,5,3]
    
    print(x)
    print(m)

    print((1,)*(ndim-1)+(3,))
    m = tensorflow.cast(-np.array(m),tensorflow.float32)
    m = tensorflow.reshape(m,(1,)*(ndim-1)+(3,))
    x += m
    print(x)
    sd = [1,0.5,0.3]
    sd = tensorflow.cast(np.array(sd),tensorflow.float32)
    sd = tensorflow.reshape(sd,(1,)*(ndim-1)+(3,))
    x /= sd
    print(x)

def t2():
    config = {'group':'test',
                'loss_func':'categorical_crossentropy',
                'data_name':'cifar10',
                'data_split':[0.9,0.1,0],
                'acc_sample_weight':None,
                'optimizer':None,
                'momentum':0.9,
                'dropout':0.0,
                'lr':0.1,
                'lr_decay_params': {'lr_decay_rate':0.1,'lr_decay_epochs_percent':[0.5,0.75]},
                'lr_decay_type':'percentage_step_decay', #fixed, exp_decay, percentage_step_decay
                'batch_size':2,
                'label_smoothing':None,
                'model_init_type':None,
                'model_name':'PA_ResNet18',
                'model_vars': None, #var = [max_features,sequence_length,embedding_dim]
                'num_classes':10,
                'img_size':(32,32,3),
                'rho':None, # radius of ball 
                'rho_decay':1, # 1 = no decay
                'm':None, # must be less than batch size
                'augs': {'flip':'horizontal','crop':4,"normalise":'resnet50'},#{'flip':horizonatal,"crop":padding},
                'weight_reg':0.0,
                'epochs': 200,
                }
    data = FData.Data(config)
    data.build_data()

    for step, (imgs, labels) in enumerate(data.train_data):
        if step == 0:
            print(imgs)
            print(labels)
            print('----')
        else:
            break
        
    for step, (imgs, labels) in enumerate(data.train_data):
        if step == 0:
            print(imgs)
            print(labels)
            print('----')
        else:
            break  
        


if __name__ == "__main__":
    #os.environ['WANDB_API_KEY'] = 'fc2ea89618ca0e1b85a71faee35950a78dd59744'
    #wandb.login()
    #wandb.init(project="SAM")
    #training_process()
    t2()





